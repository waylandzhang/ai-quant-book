# 背景知识：前沿 ML 与 RL 方法（2025）

> 本文梳理 2025 年量化交易领域最前沿的机器学习和强化学习方法。这些是顶级量化机构（Two Sigma、Citadel、幻方、九坤）正在探索或已在使用的技术。

---

## 一、技术演进概览

### 1.1 从传统到前沿

| 代际 | 代表技术 | 状态 |
|-----|---------|------|
| 第一代 | 线性回归、Logistic | 基础，仍在使用 |
| 第二代 | LSTM、GRU | 仍有应用场景（低延迟、小数据），但主流关注已转向 Transformer |
| 第三代 | Transformer、GNN | 当前主流 |
| 第四代 | Foundation Models、Diffusion | **前沿探索** |

> **注意**：LSTM/GRU 并非完全过时。在低延迟场景（<1ms推理）、小数据集、或简单时序预测中，它们仍是合理选择。详见[模型架构选择指南](模型架构选择指南.md)。

### 1.2 头部机构技术布局

| 机构 | 公开技术方向 | 算力投入 |
|-----|------------|---------|
| 幻方量化 | DeepSeek大模型、萤火二号AI集群 | 10亿+元 |
| 九坤投资 | 与微软合作垂直场景AI | 未披露 |
| Two Sigma | 数据科学+大规模ML | 600亿美元AUM支撑 |
| Citadel | 高频交易基础设施+AI | 持续招聘AI人才 |

---

## 二、Decision Transformer（决策 Transformer）

### 2.1 核心思想

**将强化学习问题转化为序列建模问题**：

```
传统RL: State → Policy → Action → Reward → 更新Policy
Decision Transformer: (Return, State, Action)序列 → 下一个Action
```

**关键创新**：
- 不需要值函数估计
- 不需要策略梯度
- 直接用Transformer建模"如果我想要这个收益，应该怎么做"

### 2.2 GPT-2 + LoRA 用于交易

**最新研究（2024年11月）**：

```
架构:
Pre-trained GPT-2
    ↓
LoRA 微调 (Low-Rank Adaptation)
    ↓
Decision Transformer for Trading
```

**为什么有效**：
- GPT-2 的预训练权重提供强大的序列建模能力
- LoRA 仅微调少量参数（~0.1%），高效且防止过拟合
- 适合金融数据稀缺的场景

**性能**：与 CQL、IQL、BC 等离线RL算法竞争力相当，在某些场景下更优

### 2.3 TACR（Transformer Actor-Critic with Regularization）

**解决问题**：传统RL假设马尔可夫性（只看当前状态），但金融市场有长期依赖

**方法**：用 Decision Transformer 的注意力机制建模历史 MDP 序列

**延伸练习**：实现一个简单的 Decision Transformer 交易框架

---

## 三、LLM 驱动的 Alpha 挖掘

### 3.1 AlphaAgent 框架

**核心思想**：多智能体协作挖掘 Alpha 因子

**架构**：

```
┌─────────────────────────────────────────────────────┐
│                   AlphaAgent 系统                    │
├─────────────────────────────────────────────────────┤
│  Research Agent     →  生成因子假设                  │
│       ↓                                             │
│  Backtest Agent     →  验证因子有效性                │
│       ↓                                             │
│  Risk Agent         →  评估因子风险特性              │
│       ↓                                             │
│  Portfolio Agent    →  组合优化与权重分配            │
└─────────────────────────────────────────────────────┘
```

**关键特点**：
- **多智能体分工**：每个 Agent 专注单一任务，避免单一 LLM 的能力瓶颈
- **迭代优化**：通过回测反馈持续改进因子
- **风险意识**：Risk Agent 内置于流程中，非事后检查
- **可解释性**：每个决策节点都有清晰的推理链

**与传统方法对比**：

| 特性 | 传统量化 | 单一 LLM | AlphaAgent |
|-----|---------|----------|------------|
| 因子挖掘效率 | 低（人工） | 中 | 高 |
| 风险控制 | 事后 | 弱 | 内置 |
| 可解释性 | 高 | 低 | 高 |
| 迭代速度 | 慢 | 快 | 快 |

### 3.2 LLM-Guided RL

**来源**：arXiv 2508.02366（2025年）

**核心思想**：

```
LLM: 生成高层策略（"市场处于上涨趋势，建议超配科技股"）
 ↓
RL Agent: 执行具体交易（"买入AAPL 100股，限价$185"）
 ↓
Reward: 反馈给LLM改进策略
```

**优势**：
- LLM 提供可解释的高层逻辑
- RL 优化低层执行细节
- 两者互补

**实验结果**：在6只股票中，4只的夏普比率优于纯RL基线

### 3.3 Alpha-GPT 2.0

**定位**：Human-in-the-Loop AI

**流程**：
1. LLM 生成因子候选
2. 人类专家审查/修改
3. 回测验证
4. 反馈改进

**适用场景**：需要人工把控的机构级应用

**延伸练习**：实现一个简单的 LLM 因子生成 pipeline

---

## 四、图神经网络（GNN）

### 4.1 为什么需要 GNN

**传统方法的局限**：
- 假设股票独立
- 忽略关联关系

**市场现实**：
- 供应链关系（Apple → 台积电）
- 行业关联（银行股同涨同跌）
- 宏观因子（利率影响所有股票）

### 4.2 Role-Aware Graph Transformer

**来源**：2025年12月研究

**多关系建模**：

| 边类型 | 含义 | 构建方式 |
|-------|------|---------|
| Correlation | 价格相关性 | 历史收益率相关系数 |
| Fundamental | 基本面相似 | PE、PB、ROE等 |
| Sector | 行业关系 | GICS分类 |
| Supply Chain | 供应链 | 财报披露 |

**角色感知**：
- Hub Stocks（如AAPL、MSFT）：影响很多其他股票
- Bridge Stocks：连接不同行业
- Peripheral Stocks：被动跟随

### 4.3 TFT-GNN 混合模型

**Temporal Fusion Transformer + Graph Neural Network**

```
时间维度: TFT 捕捉
    ↓
关系维度: GNN 建模
    ↓
融合层
    ↓
预测
```

**性能**：MSE 降低 10.6%（对比单独TFT）

**延伸练习**：实现一个简单的股票关系图构建和 GNN 预测

---

## 五、扩散模型（Diffusion Models）

### 5.1 应用场景

| 场景 | 传统方法 | 扩散模型优势 |
|-----|---------|------------|
| 合成数据生成 | GAN | 更稳定，无模式崩塌 |
| 市场模拟 | 蒙特卡洛 | 更真实的统计特性 |
| LOB模拟 | 规则模型 | 捕捉复杂动态 |

### 5.2 TRADES 框架

**来源**：arXiv 2502.07071（2025年2月）

**定位**：TRAnsformer-based Denoising Diffusion for LOB Simulations

**架构**：
```
Limit Order Book State
    ↓
Transformer Encoder（捕捉时空特征）
    ↓
DDPM（去噪扩散）
    ↓
生成的订单流
```

**性能**：Predictive Score 提升 3.27x（对比SOTA）

**开源**：DeepMarket（首个开源LOB深度学习模拟框架）

### 5.3 Wavelet + DDPM 方法

**来源**：arXiv 2410.18897

**创新**：将时间序列转为图像

```
多时间序列（价格、成交量、价差）
    ↓
小波变换 → 图像
    ↓
DDPM 生成新图像
    ↓
逆小波变换 → 合成时间序列
```

**优势**：
- 捕捉金融数据的 stylized facts（肥尾、波动聚集）
- 生成质量优于 GAN
- 可用于回测数据增强

### 5.4 应用价值

| 应用 | 说明 |
|-----|------|
| 数据增强 | 扩充稀缺的历史数据 |
| 压力测试 | 生成极端市场场景 |
| 回测稳健性 | 多场景验证策略 |
| 隐私保护 | 生成合成数据替代真实数据 |

**延伸练习**：研究 TRADES 框架的可用性，评估是否可以集成

---

## 六、时间序列基础模型

### 6.1 概览

| 模型 | 开发者 | 参数量 | 特点 |
|-----|-------|-------|------|
| Chronos-2 | Amazon | 120M | 最新（2025年10月） |
| TimeGPT | Nixtla | - | 100B+ tokens 训练 |
| TimesFM | Google | - | - |
| Moirai | Salesforce | - | - |

### 6.2 Chronos-2

**发布**：2025年10月20日

**能力**：
- 零样本预测（无需微调）
- 单变量 / 多变量 / 协变量
- 单一架构支持所有场景

**下载量**：600M+（Hugging Face）

### 6.3 金融应用注意事项

**研究发现**：
- 通用基础模型在金融领域效果有限
- 领域对齐的模型（如 FinCast）表现更好
- 金融数据的低信噪比是主要挑战

**建议**：
- 作为基线参考
- 可能需要金融数据微调
- 不建议直接用于生产信号

**延伸练习**：评估 Chronos-2 在股票预测任务上的零样本效果

---

## 七、强化学习前沿

### 7.1 算法选择指南（2025）

| 场景 | 推荐算法 | 原因 |
|-----|---------|------|
| 投资组合配置 | **PPO** | 连续动作空间，稳定 |
| 订单执行优化 | **SAC** | 探索性强，适应波动 |
| 离散交易决策 | **DQN** | 简单有效 |
| 风险感知投资 | **QR-DDPG** | 分位数回归捕捉尾部风险 |

### 7.2 Hybrid Approaches 趋势

**2025年数据**：
- 混合方法采用率：42%（2020年仅15%）
- 纯RL采用率：58%（2020年85%）

**混合优势**：
| 组合 | 应用 | 提升 |
|-----|------|-----|
| LSTM-DQN | 投资组合优化 | +15.4% |
| CNN-PPO | 加密货币交易 | +17.9% |
| Attention-DDPG | 做市 | +16.3% |

### 7.3 IMM（Imitative Market Maker）

**来源**：IJCAI 2024

**创新**：
- 多价格水平订单簿建模
- 模仿学习（从专家做市商学习）
- 结合专家信号

**应用**：做市策略的RL优化

### 7.4 FinRL框架

**定位**：金融强化学习的开源标准框架

**特点**：
- 基于 OpenAI Gym 的标准化环境
- 集成 DQN、PPO、A3C、SAC 等算法
- 支持回测和风险评估

**推荐使用**：作为 RL 策略开发的起点

**延伸练习**：评估 FinRL 集成到现有框架的可行性

---

## 八、多智能体系统

### 8.1 动态门控架构

**核心思想**：

![多智能体动态门控架构](../assets/multi-agent-dynamic-gating.svg)

**优势**：
- 每个 Agent 专注特定市场状态
- 避免单一模型过拟合
- 动态适应市场变化

### 8.2 FinMem

**定位**：带分层记忆的 LLM 交易 Agent

**记忆结构**：
- 短期记忆：近期市场事件
- 工作记忆：当前持仓和决策上下文
- 长期记忆：历史模式和经验教训

### 8.3 TwinMarket

**来源**：Yang et al. 2025

**特点**：模拟市场中的个体行为和集体动态

**应用**：
- 研究金融泡沫形成
- 理解市场涌现现象
- 策略在复杂市场中的表现测试

**延伸练习**：研究多智能体门控机制的实现

---

## 九、实践路线图

### 9.1 优先级排序

| 优先级 | 技术 | 理由 |
|-------|------|------|
| P0 | LLM-Guided RL | 可解释性 + 性能 |
| P0 | Chain-of-Alpha | 自动化因子挖掘 |
| P1 | GNN 关系建模 | 捕捉市场结构 |
| P1 | Decision Transformer | 替代传统RL |
| P2 | 扩散模型 | 数据增强/压力测试 |
| P2 | 时间序列基础模型 | 零样本预测能力 |

### 9.2 实施建议

**短期（1-3个月）**：
- 评估 FinRL 框架
- 实现简单的 LLM 因子生成 pipeline
- 构建股票关系图

**中期（3-6个月）**：
- 实现 Decision Transformer 框架
- 集成 GNN 进行关系预测
- 开发多智能体门控系统

**长期（6-12个月）**：
- 完整的 Chain-of-Alpha 系统
- 扩散模型用于数据增强
- 生产级部署和监控

---

## 十、参考资源

### 论文
- Chain-of-Alpha: arXiv 2508.06312
- LLM-Guided RL: arXiv 2508.02366
- Decision Transformer for Trading: arXiv 2411.17900
- TRADES: arXiv 2502.07071
- GNN Survey for Stock: ACM Computing Surveys 2024
- RL in Finance Review: arXiv 2512.10913

### 开源框架
- FinRL: https://github.com/AI4Finance-Foundation/FinRL
- DeepMarket: （随TRADES论文发布）
- Chronos-2: https://huggingface.co/amazon/chronos-2
- FinGPT: https://github.com/AI4Finance-Foundation/FinGPT

### 数据集
- FinRL Contest 数据集
- LOBSTER（学术LOB数据）

---

> **核心原则**：追踪前沿，但不盲目追新。每项技术都需要在你的具体场景中验证，而非照搬论文结论。头部机构的优势在于能够大规模试错和迭代，而非使用了某个"神奇"的模型。
